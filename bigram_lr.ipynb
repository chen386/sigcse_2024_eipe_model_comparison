{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b61e0e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import itertools\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from spellchecker import SpellChecker\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b13a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unescape_response(response):\n",
    "    response = unidecode(response)\n",
    "    prev_response = response\n",
    "    response = html.unescape(response)\n",
    "    while prev_response != response:\n",
    "        prev_response = response\n",
    "        response = html.unescape(response)\n",
    "\n",
    "    return response\n",
    "\n",
    "def tokenize(response):\n",
    "    response = str(response)\n",
    "    \n",
    "    spell_checker = SpellChecker()\n",
    "    spell_checker.distance = 1\n",
    "    \n",
    "    punctuations = ' ,.:;?'\n",
    "    preprocessed_response = []\n",
    "    for i in range(len(response)):\n",
    "        if response[i] in [\"'\", '\"']:\n",
    "            if i != 0 and i != len(response) - 1:\n",
    "                if response[i - 1] not in punctuations and response[i + 1] not in punctuations:\n",
    "                    preprocessed_response.append(response[i])\n",
    "        else:\n",
    "            preprocessed_response.append(response[i])\n",
    "    response = ''.join(preprocessed_response)\n",
    "\n",
    "    operators = '+-*/%=<>()[]{}#'\n",
    "    preprocessed_response = []\n",
    "    for i in range(len(response)):\n",
    "        if response[i] in operators:\n",
    "            if i != 0 and response[i - 1] != ' ':\n",
    "                preprocessed_response.append(' ')\n",
    "            preprocessed_response.append(response[i])\n",
    "            if i != len(response) - 1 and response[i + 1] != ' ':\n",
    "                preprocessed_response.append(' ')\n",
    "        else:\n",
    "            preprocessed_response.append(response[i])\n",
    "\n",
    "    response = ''.join(preprocessed_response)\n",
    "\n",
    "    spell_checker_skip = [\"it's\"]\n",
    "    tokens_list = []\n",
    "    for sentence in nltk.sent_tokenize(response):\n",
    "        nltk_tokens = nltk.word_tokenize(sentence)\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(nltk_tokens):\n",
    "            if i < len(nltk_tokens) - 1 and nltk_tokens[i + 1][0] == \"'\":\n",
    "                tokens.append(nltk_tokens[i].lower() + nltk_tokens[i + 1].lower())\n",
    "                i += 1\n",
    "            elif nltk_tokens[i] not in punctuations:\n",
    "                tokens.append(nltk_tokens[i].lower())\n",
    "            i += 1\n",
    "        \n",
    "        tokens_list.append([(spell_checker.correction(token) if spell_checker.correction(token) is not None else token) \n",
    "                            if token not in spell_checker_skip else token \n",
    "                            for token in tokens])\n",
    "\n",
    "    return tokens_list\n",
    "\n",
    "def initialize_bow(tokens_lists, percentage_threshold=0.6):\n",
    "    token_to_count = {}\n",
    "    for token_list in tokens_lists:\n",
    "        tokens = set(itertools.chain.from_iterable(token_list))\n",
    "        for token in tokens:\n",
    "            if token not in token_to_count:\n",
    "                token_to_count[token] = 0\n",
    "            token_to_count[token] += 1\n",
    "            \n",
    "    sorted_counts = sorted(token_to_count.values())\n",
    "    threshold = sorted_counts[int(len(sorted_counts) * percentage_threshold)]\n",
    "    \n",
    "    token_to_index = {}\n",
    "    index = 0\n",
    "    for token in token_to_count:\n",
    "        if token_to_count[token] >= threshold:\n",
    "            token_to_index[token] = index\n",
    "            index += 1\n",
    "    \n",
    "    return token_to_index, len(token_to_index)\n",
    "\n",
    "def bowize(token_to_index, tokens_list):\n",
    "    tokens = set(itertools.chain.from_iterable(tokens_list))\n",
    "    \n",
    "    X = np.zeros((1, len(token_to_index)))\n",
    "    for token in tokens:\n",
    "        if token in token_to_index:\n",
    "            X[0, token_to_index[token]] = 1\n",
    "    \n",
    "    return X\n",
    "\n",
    "def initialize_bigram(tokens_lists, percentage_threshold=0.8):\n",
    "    bigram_to_count = {}\n",
    "    for tokens_list in tokens_lists:\n",
    "        bigrams = []\n",
    "        for tokens in tokens_list:\n",
    "            bigrams.extend(map(lambda x : x[0] + ' ' + x[1], zip(tokens[0:-1], tokens[1:])))\n",
    "        \n",
    "        bigrams = set(bigrams)\n",
    "        for bigram in bigrams:\n",
    "            if bigram not in bigram_to_count:\n",
    "                bigram_to_count[bigram] = 0\n",
    "            bigram_to_count[bigram] += 1\n",
    "    \n",
    "    sorted_counts = sorted(bigram_to_count.values())\n",
    "    threshold = sorted_counts[int(len(sorted_counts) * percentage_threshold)]\n",
    "    \n",
    "    bigram_to_index = {}\n",
    "    index = 0\n",
    "    for bigram in bigram_to_count:\n",
    "        if bigram_to_count[bigram] >= threshold:\n",
    "            bigram_to_index[bigram] = index\n",
    "            index += 1\n",
    "    \n",
    "    token_to_index, _ = initialize_bow(tokens_lists)\n",
    "    \n",
    "    return (token_to_index, bigram_to_index), len(token_to_index) + len(bigram_to_index)\n",
    "    \n",
    "def bigramize(to_index_tuple, tokens_list):\n",
    "    token_to_index, bigram_to_index = to_index_tuple\n",
    "    \n",
    "    X = np.zeros((1, len(token_to_index) + len(bigram_to_index)))\n",
    "    \n",
    "    tokens = set(itertools.chain.from_iterable(tokens_list))\n",
    "    for token in tokens:\n",
    "        if token in token_to_index:\n",
    "            X[0, token_to_index[token]] = 1\n",
    "    \n",
    "    for tokens in tokens_list:\n",
    "        for bigram in map(lambda x : x[0] + ' ' + x[1], zip(tokens[0:-1], tokens[1:])):\n",
    "            if bigram in bigram_to_index:\n",
    "                X[0, len(token_to_index) + bigram_to_index[bigram]] = 1\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6d66e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sigcse_2024.csv')\n",
    "\n",
    "train_df = df[df.subset == 'train'].copy()\n",
    "test_df = df[df.subset == 'test'].copy()\n",
    "\n",
    "train_df['tokens_list'] = train_df.response.apply(tokenize)\n",
    "test_df['tokens_list'] = test_df.response.apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cd24731",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df['bigram_lr'] = 0\n",
    "for qid, sub_train_df in train_df.groupby('qid'):\n",
    "    lr = LogisticRegression()\n",
    "    \n",
    "    feature_map, feature_size = initialize_bigram(sub_train_df.tokens_list)\n",
    "    \n",
    "    train_X = np.zeros((len(sub_train_df), feature_size))\n",
    "    train_y = np.zeros((len(sub_train_df), ), dtype=int)\n",
    "    index = 0\n",
    "    for _, row in sub_train_df.iterrows():\n",
    "        train_X[index, :] = bigramize(feature_map, row.tokens_list)\n",
    "        train_y[index] = row.binary_ground_truth\n",
    "\n",
    "        index += 1\n",
    "\n",
    "    lr.fit(train_X, train_y)\n",
    "    \n",
    "    sub_test_df = test_df[test_df.qid == qid]\n",
    "    \n",
    "    test_X = np.zeros((len(sub_test_df), feature_size))\n",
    "    test_y = np.zeros((len(sub_test_df), ), dtype=int)\n",
    "    index = 0\n",
    "    for _, row in sub_test_df.iterrows():\n",
    "        test_X[index, :] = bigramize(feature_map, row.tokens_list)\n",
    "        test_y[index] = row.binary_ground_truth\n",
    "\n",
    "        index += 1\n",
    "    \n",
    "    targets = np.array(test_y)\n",
    "    predicted = lr.predict(test_X)\n",
    "    \n",
    "    test_df.loc[sub_test_df.index, 'bigram_lr'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "077a8935",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(list(test_df.bigram_lr), open('bigram_lr.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
