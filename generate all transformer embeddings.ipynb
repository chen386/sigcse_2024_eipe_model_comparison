{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26607e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/projects/chen386/.cache/huggingface'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c919885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from transformers import  AutoModel, AutoTokenizer, BertModel, LlamaModel, LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03efc054",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1be24046",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sigcse_2024.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d7464f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_token(model_output):\n",
    "    token_embeddings = model_output[0]\n",
    "    return token_embeddings[0][-1].view(1, len(token_embeddings[0][-1]))\n",
    "\n",
    "# copied from huggingface sbert example \n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5897fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_to_meta_data = {\n",
    "    'bert_base' : {\n",
    "        'tokenizer_class' : AutoTokenizer,\n",
    "        'tokenizer_path' : 'bert-base-uncased',\n",
    "        'model_class' : BertModel,\n",
    "        'model_path' : 'bert-base-uncased',\n",
    "        'load_in_4bit' : False\n",
    "    },\n",
    "    'bert_large' : {\n",
    "        'tokenizer_class' : AutoTokenizer,\n",
    "        'tokenizer_path' : 'bert-large-uncased',\n",
    "        'model_class' : BertModel,\n",
    "        'model_path' : 'bert-large-uncased',\n",
    "        'load_in_4bit' : False\n",
    "    },\n",
    "    'sbert' : {\n",
    "        'tokenizer_class' : AutoTokenizer,\n",
    "        'tokenizer_path' : 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "        'model_class' : AutoModel,\n",
    "        'model_path' : 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "        'load_in_4bit' : False\n",
    "    },\n",
    "    'gpt2' : {\n",
    "        'tokenizer_class' : AutoTokenizer,\n",
    "        'tokenizer_path' : 'gpt2',\n",
    "        'model_class' : AutoModel,\n",
    "        'model_path' : 'gpt2',\n",
    "        'load_in_4bit' : False\n",
    "    },\n",
    "    'gpt2_medium' : {\n",
    "        'tokenizer_class' : AutoTokenizer,\n",
    "        'tokenizer_path' : 'gpt2-medium',\n",
    "        'model_class' : AutoModel,\n",
    "        'model_path' : 'gpt2-medium',\n",
    "        'load_in_4bit' : False\n",
    "    },\n",
    "    'gpt2_large' : {\n",
    "        'tokenizer_class' : AutoTokenizer,\n",
    "        'tokenizer_path' : 'gpt2-large',\n",
    "        'model_class' : AutoModel,\n",
    "        'model_path' : 'gpt2-large',\n",
    "        'load_in_4bit' : False\n",
    "    },\n",
    "    'gpt2_xl' : {\n",
    "        'tokenizer_class' : AutoTokenizer,\n",
    "        'tokenizer_path' : 'gpt2-xl',\n",
    "        'model_class' : AutoModel,\n",
    "        'model_path' : 'gpt2-xl',\n",
    "        'load_in_4bit' : False\n",
    "    },\n",
    "    'llama_7b' : {\n",
    "        'tokenizer_class' : LlamaTokenizer,\n",
    "        'tokenizer_path' : '7B',\n",
    "        'model_class' : LlamaModel,\n",
    "        'model_path' : '7B',\n",
    "        'load_in_4bit' : True\n",
    "    },\n",
    "    'llama_13b' : {\n",
    "        'tokenizer_class' : LlamaTokenizer,\n",
    "        'tokenizer_path' : '13B',\n",
    "        'model_class' : LlamaModel,\n",
    "        'model_path' : '13B',\n",
    "        'load_in_4bit' : True\n",
    "    },\n",
    "    'llama_30b' : {\n",
    "        'tokenizer_class' : LlamaTokenizer,\n",
    "        'tokenizer_path' : '30B',\n",
    "        'model_class' : LlamaModel,\n",
    "        'model_path' : '30B',\n",
    "        'load_in_4bit' : True\n",
    "    },\n",
    "    'llama_65b' : {\n",
    "        'tokenizer_class' : LlamaTokenizer,\n",
    "        'tokenizer_path' : '65B',\n",
    "        'model_class' : LlamaModel,\n",
    "        'model_path' : '65B',\n",
    "        'load_in_4bit' : True\n",
    "    },\n",
    "    'llama2_7b' : {\n",
    "        'tokenizer_class' : AutoTokenizer,\n",
    "        'tokenizer_path' : 'meta-llama/Llama-2-7b-hf',\n",
    "        'model_class' : AutoModel,\n",
    "        'model_path' : 'meta-llama/Llama-2-7b-hf',\n",
    "        'load_in_4bit' : True\n",
    "    },\n",
    "    'llama2_7b_chat' : {\n",
    "        'tokenizer_class' : AutoTokenizer,\n",
    "        'tokenizer_path' : 'meta-llama/Llama-2-7b-chat-hf',\n",
    "        'model_class' : AutoModel,\n",
    "        'model_path' : 'meta-llama/Llama-2-7b-chat-hf',\n",
    "        'load_in_4bit' : True\n",
    "    },\n",
    "    'llama2_13b' : {\n",
    "        'tokenizer_class' : AutoTokenizer,\n",
    "        'tokenizer_path' : 'meta-llama/Llama-2-13b-hf',\n",
    "        'model_class' : AutoModel,\n",
    "        'model_path' : 'meta-llama/Llama-2-13b-hf',\n",
    "        'load_in_4bit' : True\n",
    "    },\n",
    "    'llama2_13b_chat' : {\n",
    "        'tokenizer_class' : AutoTokenizer,\n",
    "        'tokenizer_path' : 'meta-llama/Llama-2-13b-chat-hf',\n",
    "        'model_class' : AutoModel,\n",
    "        'model_path' : 'meta-llama/Llama-2-13b-chat-hf',\n",
    "        'load_in_4bit' : True\n",
    "    },\n",
    "    'llama2_70b' : {\n",
    "        'tokenizer_class' : AutoTokenizer,\n",
    "        'tokenizer_path' : 'meta-llama/Llama-2-70b-hf',\n",
    "        'model_class' : AutoModel,\n",
    "        'model_path' : 'meta-llama/Llama-2-70b-hf',\n",
    "        'load_in_4bit' : True\n",
    "    },\n",
    "    'llama2_70b_chat' : {\n",
    "        'tokenizer_class' : AutoTokenizer,\n",
    "        'tokenizer_path' : 'meta-llama/Llama-2-70b-chat-hf',\n",
    "        'model_class' : AutoModel,\n",
    "        'model_path' : 'meta-llama/Llama-2-70b-chat-hf',\n",
    "        'load_in_4bit' : True\n",
    "    },\n",
    "    'vicuna_7b' : {\n",
    "        'tokenizer_class' : AutoTokenizer,\n",
    "        'tokenizer_path' : 'lmsys/vicuna-7b-v1.3',\n",
    "        'model_class' : AutoModel,\n",
    "        'model_path' : 'lmsys/vicuna-7b-v1.3',\n",
    "        'load_in_4bit' : True\n",
    "    },\n",
    "    'vicuna_13b' : {\n",
    "        'tokenizer_class' : AutoTokenizer,\n",
    "        'tokenizer_path' : 'lmsys/vicuna-13b-v1.3',\n",
    "        'model_class' : AutoModel,\n",
    "        'model_path' : 'lmsys/vicuna-13b-v1.3',\n",
    "        'load_in_4bit' : True\n",
    "    },\n",
    "    'vicuna_33b' : {\n",
    "        'tokenizer_class' : AutoTokenizer,\n",
    "        'tokenizer_path' : 'lmsys/vicuna-33b-v1.3',\n",
    "        'model_class' : AutoModel,\n",
    "        'model_path' : 'lmsys/vicuna-33b-v1.3',\n",
    "        'load_in_4bit' : True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81ef1491",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_base\n",
      "0/3064\n",
      "100/3064\n",
      "200/3064\n",
      "300/3064\n",
      "400/3064\n",
      "500/3064\n",
      "600/3064\n",
      "700/3064\n",
      "800/3064\n",
      "900/3064\n",
      "1000/3064\n",
      "1100/3064\n",
      "1200/3064\n",
      "1300/3064\n",
      "1400/3064\n",
      "1500/3064\n",
      "1600/3064\n",
      "1700/3064\n",
      "1800/3064\n",
      "1900/3064\n",
      "2000/3064\n",
      "2100/3064\n",
      "2200/3064\n",
      "2300/3064\n",
      "2400/3064\n",
      "2500/3064\n",
      "2600/3064\n",
      "2700/3064\n",
      "2800/3064\n",
      "2900/3064\n",
      "3000/3064\n",
      "(3064, 768)\n",
      "0.8643790849673203\n",
      "bert_large\n",
      "0/3064\n",
      "100/3064\n",
      "200/3064\n",
      "300/3064\n",
      "400/3064\n",
      "500/3064\n",
      "600/3064\n",
      "700/3064\n",
      "800/3064\n",
      "900/3064\n",
      "1000/3064\n",
      "1100/3064\n",
      "1200/3064\n",
      "1300/3064\n",
      "1400/3064\n",
      "1500/3064\n",
      "1600/3064\n",
      "1700/3064\n",
      "1800/3064\n",
      "1900/3064\n",
      "2000/3064\n",
      "2100/3064\n",
      "2200/3064\n",
      "2300/3064\n",
      "2400/3064\n",
      "2500/3064\n",
      "2600/3064\n",
      "2700/3064\n",
      "2800/3064\n",
      "2900/3064\n",
      "3000/3064\n",
      "(3064, 1024)\n",
      "0.8741830065359477\n",
      "sbert\n",
      "0/3064\n",
      "100/3064\n",
      "200/3064\n",
      "300/3064\n",
      "400/3064\n",
      "500/3064\n",
      "600/3064\n",
      "700/3064\n",
      "800/3064\n",
      "900/3064\n",
      "1000/3064\n",
      "1100/3064\n",
      "1200/3064\n",
      "1300/3064\n",
      "1400/3064\n",
      "1500/3064\n",
      "1600/3064\n",
      "1700/3064\n",
      "1800/3064\n",
      "1900/3064\n",
      "2000/3064\n",
      "2100/3064\n",
      "2200/3064\n",
      "2300/3064\n",
      "2400/3064\n",
      "2500/3064\n",
      "2600/3064\n",
      "2700/3064\n",
      "2800/3064\n",
      "2900/3064\n",
      "3000/3064\n",
      "(3064, 384)\n",
      "0.8921568627450981\n",
      "gpt2\n",
      "0/3064\n",
      "100/3064\n",
      "200/3064\n",
      "300/3064\n",
      "400/3064\n",
      "500/3064\n",
      "600/3064\n",
      "700/3064\n",
      "800/3064\n",
      "900/3064\n",
      "1000/3064\n",
      "1100/3064\n",
      "1200/3064\n",
      "1300/3064\n",
      "1400/3064\n",
      "1500/3064\n",
      "1600/3064\n",
      "1700/3064\n",
      "1800/3064\n",
      "1900/3064\n",
      "2000/3064\n",
      "2100/3064\n",
      "2200/3064\n",
      "2300/3064\n",
      "2400/3064\n",
      "2500/3064\n",
      "2600/3064\n",
      "2700/3064\n",
      "2800/3064\n",
      "2900/3064\n",
      "3000/3064\n",
      "(3064, 768)\n",
      "0.8643790849673203\n",
      "gpt2_medium\n",
      "0/3064\n",
      "100/3064\n",
      "200/3064\n",
      "300/3064\n",
      "400/3064\n",
      "500/3064\n",
      "600/3064\n",
      "700/3064\n",
      "800/3064\n",
      "900/3064\n",
      "1000/3064\n",
      "1100/3064\n",
      "1200/3064\n",
      "1300/3064\n",
      "1400/3064\n",
      "1500/3064\n",
      "1600/3064\n",
      "1700/3064\n",
      "1800/3064\n",
      "1900/3064\n",
      "2000/3064\n",
      "2100/3064\n",
      "2200/3064\n",
      "2300/3064\n",
      "2400/3064\n",
      "2500/3064\n",
      "2600/3064\n",
      "2700/3064\n",
      "2800/3064\n",
      "2900/3064\n",
      "3000/3064\n",
      "(3064, 1024)\n",
      "0.8562091503267973\n",
      "gpt2_large\n",
      "0/3064\n",
      "100/3064\n",
      "200/3064\n",
      "300/3064\n",
      "400/3064\n",
      "500/3064\n",
      "600/3064\n",
      "700/3064\n",
      "800/3064\n",
      "900/3064\n",
      "1000/3064\n",
      "1100/3064\n",
      "1200/3064\n",
      "1300/3064\n",
      "1400/3064\n",
      "1500/3064\n",
      "1600/3064\n",
      "1700/3064\n",
      "1800/3064\n",
      "1900/3064\n",
      "2000/3064\n",
      "2100/3064\n",
      "2200/3064\n",
      "2300/3064\n",
      "2400/3064\n",
      "2500/3064\n",
      "2600/3064\n",
      "2700/3064\n",
      "2800/3064\n",
      "2900/3064\n",
      "3000/3064\n",
      "(3064, 1280)\n",
      "0.8758169934640523\n",
      "gpt2_xl\n",
      "0/3064\n",
      "100/3064\n",
      "200/3064\n",
      "300/3064\n",
      "400/3064\n",
      "500/3064\n",
      "600/3064\n",
      "700/3064\n",
      "800/3064\n",
      "900/3064\n",
      "1000/3064\n",
      "1100/3064\n",
      "1200/3064\n",
      "1300/3064\n",
      "1400/3064\n",
      "1500/3064\n",
      "1600/3064\n",
      "1700/3064\n",
      "1800/3064\n",
      "1900/3064\n",
      "2000/3064\n",
      "2100/3064\n",
      "2200/3064\n",
      "2300/3064\n",
      "2400/3064\n",
      "2500/3064\n",
      "2600/3064\n",
      "2700/3064\n",
      "2800/3064\n",
      "2900/3064\n",
      "3000/3064\n",
      "(3064, 1600)\n",
      "0.8823529411764706\n",
      "llama_7b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbea5299e0549879337af542c7b821e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3064\n",
      "100/3064\n",
      "200/3064\n",
      "300/3064\n",
      "400/3064\n",
      "500/3064\n",
      "600/3064\n",
      "700/3064\n",
      "800/3064\n",
      "900/3064\n",
      "1000/3064\n",
      "1100/3064\n",
      "1200/3064\n",
      "1300/3064\n",
      "1400/3064\n",
      "1500/3064\n",
      "1600/3064\n",
      "1700/3064\n",
      "1800/3064\n",
      "1900/3064\n",
      "2000/3064\n",
      "2100/3064\n",
      "2200/3064\n",
      "2300/3064\n",
      "2400/3064\n",
      "2500/3064\n",
      "2600/3064\n",
      "2700/3064\n",
      "2800/3064\n",
      "2900/3064\n",
      "3000/3064\n",
      "(3064, 4096)\n",
      "0.8725490196078431\n",
      "llama_13b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba72188135f430790e12b9c2212a96f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3064\n",
      "100/3064\n",
      "200/3064\n",
      "300/3064\n",
      "400/3064\n",
      "500/3064\n",
      "600/3064\n",
      "700/3064\n",
      "800/3064\n",
      "900/3064\n",
      "1000/3064\n",
      "1100/3064\n",
      "1200/3064\n",
      "1300/3064\n",
      "1400/3064\n",
      "1500/3064\n",
      "1600/3064\n",
      "1700/3064\n",
      "1800/3064\n",
      "1900/3064\n",
      "2000/3064\n",
      "2100/3064\n",
      "2200/3064\n",
      "2300/3064\n",
      "2400/3064\n",
      "2500/3064\n",
      "2600/3064\n",
      "2700/3064\n",
      "2800/3064\n",
      "2900/3064\n",
      "3000/3064\n",
      "(3064, 5120)\n",
      "0.8954248366013072\n",
      "llama_30b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1049ab29e422410d868f9bb2309122b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3064\n",
      "100/3064\n",
      "200/3064\n",
      "300/3064\n",
      "400/3064\n",
      "500/3064\n",
      "600/3064\n",
      "700/3064\n",
      "800/3064\n",
      "900/3064\n",
      "1000/3064\n",
      "1100/3064\n",
      "1200/3064\n",
      "1300/3064\n",
      "1400/3064\n",
      "1500/3064\n",
      "1600/3064\n",
      "1700/3064\n",
      "1800/3064\n",
      "1900/3064\n",
      "2000/3064\n",
      "2100/3064\n",
      "2200/3064\n",
      "2300/3064\n",
      "2400/3064\n",
      "2500/3064\n",
      "2600/3064\n",
      "2700/3064\n",
      "2800/3064\n",
      "2900/3064\n",
      "3000/3064\n",
      "(3064, 6656)\n",
      "0.8856209150326797\n",
      "llama_65b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f85c935fb24458ebfcea332c00463de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3064\n",
      "100/3064\n",
      "200/3064\n",
      "300/3064\n",
      "400/3064\n",
      "500/3064\n",
      "600/3064\n",
      "700/3064\n",
      "800/3064\n",
      "900/3064\n",
      "1000/3064\n",
      "1100/3064\n",
      "1200/3064\n",
      "1300/3064\n",
      "1400/3064\n",
      "1500/3064\n",
      "1600/3064\n",
      "1700/3064\n",
      "1800/3064\n",
      "1900/3064\n",
      "2000/3064\n",
      "2100/3064\n",
      "2200/3064\n",
      "2300/3064\n",
      "2400/3064\n",
      "2500/3064\n",
      "2600/3064\n",
      "2700/3064\n",
      "2800/3064\n",
      "2900/3064\n",
      "3000/3064\n",
      "(3064, 8192)\n",
      "0.8758169934640523\n",
      "llama2_7b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef3029ad9454d48917522c36bcd4ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3064\n",
      "100/3064\n",
      "200/3064\n",
      "300/3064\n",
      "400/3064\n",
      "500/3064\n",
      "600/3064\n",
      "700/3064\n",
      "800/3064\n",
      "900/3064\n",
      "1000/3064\n",
      "1100/3064\n",
      "1200/3064\n",
      "1300/3064\n",
      "1400/3064\n",
      "1500/3064\n",
      "1600/3064\n",
      "1700/3064\n",
      "1800/3064\n",
      "1900/3064\n",
      "2000/3064\n",
      "2100/3064\n",
      "2200/3064\n",
      "2300/3064\n",
      "2400/3064\n",
      "2500/3064\n",
      "2600/3064\n",
      "2700/3064\n",
      "2800/3064\n",
      "2900/3064\n",
      "3000/3064\n",
      "(3064, 4096)\n",
      "0.8823529411764706\n",
      "llama2_7b_chat\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6566a4ac109447189f4ac7a09c36a139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3064\n",
      "100/3064\n",
      "200/3064\n",
      "300/3064\n",
      "400/3064\n",
      "500/3064\n",
      "600/3064\n",
      "700/3064\n",
      "800/3064\n",
      "900/3064\n",
      "1000/3064\n",
      "1100/3064\n",
      "1200/3064\n",
      "1300/3064\n",
      "1400/3064\n",
      "1500/3064\n",
      "1600/3064\n",
      "1700/3064\n",
      "1800/3064\n",
      "1900/3064\n",
      "2000/3064\n",
      "2100/3064\n",
      "2200/3064\n",
      "2300/3064\n",
      "2400/3064\n",
      "2500/3064\n",
      "2600/3064\n",
      "2700/3064\n",
      "2800/3064\n",
      "2900/3064\n",
      "3000/3064\n",
      "(3064, 4096)\n",
      "0.8905228758169934\n",
      "llama2_13b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f2e76e3d3a44d9bf643079b7de3a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3064\n",
      "100/3064\n",
      "200/3064\n",
      "300/3064\n",
      "400/3064\n",
      "500/3064\n",
      "600/3064\n",
      "700/3064\n",
      "800/3064\n",
      "900/3064\n",
      "1000/3064\n",
      "1100/3064\n",
      "1200/3064\n",
      "1300/3064\n",
      "1400/3064\n",
      "1500/3064\n",
      "1600/3064\n",
      "1700/3064\n",
      "1800/3064\n",
      "1900/3064\n",
      "2000/3064\n",
      "2100/3064\n",
      "2200/3064\n",
      "2300/3064\n",
      "2400/3064\n",
      "2500/3064\n",
      "2600/3064\n",
      "2700/3064\n",
      "2800/3064\n",
      "2900/3064\n",
      "3000/3064\n",
      "(3064, 5120)\n",
      "0.8839869281045751\n",
      "llama2_13b_chat\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6adf5f34c847ea84efe1a0e2b8f0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3064\n",
      "100/3064\n",
      "200/3064\n",
      "300/3064\n",
      "400/3064\n",
      "500/3064\n",
      "600/3064\n",
      "700/3064\n",
      "800/3064\n",
      "900/3064\n",
      "1000/3064\n",
      "1100/3064\n",
      "1200/3064\n",
      "1300/3064\n",
      "1400/3064\n",
      "1500/3064\n",
      "1600/3064\n",
      "1700/3064\n",
      "1800/3064\n",
      "1900/3064\n",
      "2000/3064\n",
      "2100/3064\n",
      "2200/3064\n",
      "2300/3064\n",
      "2400/3064\n",
      "2500/3064\n",
      "2600/3064\n",
      "2700/3064\n",
      "2800/3064\n",
      "2900/3064\n",
      "3000/3064\n",
      "(3064, 5120)\n",
      "0.8921568627450981\n",
      "llama2_70b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d18bef363124c5dac6112a1455cadcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3064\n",
      "100/3064\n",
      "200/3064\n",
      "300/3064\n",
      "400/3064\n",
      "500/3064\n",
      "600/3064\n",
      "700/3064\n",
      "800/3064\n",
      "900/3064\n",
      "1000/3064\n",
      "1100/3064\n",
      "1200/3064\n",
      "1300/3064\n",
      "1400/3064\n",
      "1500/3064\n",
      "1600/3064\n",
      "1700/3064\n",
      "1800/3064\n",
      "1900/3064\n",
      "2000/3064\n",
      "2100/3064\n",
      "2200/3064\n",
      "2300/3064\n",
      "2400/3064\n",
      "2500/3064\n",
      "2600/3064\n",
      "2700/3064\n",
      "2800/3064\n",
      "2900/3064\n",
      "3000/3064\n",
      "(3064, 8192)\n",
      "0.8709150326797386\n",
      "llama2_70b_chat\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a43c7560a5a4568834cbfa953ba19b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3064\n",
      "100/3064\n",
      "200/3064\n",
      "300/3064\n",
      "400/3064\n",
      "500/3064\n",
      "600/3064\n",
      "700/3064\n",
      "800/3064\n",
      "900/3064\n",
      "1000/3064\n",
      "1100/3064\n",
      "1200/3064\n",
      "1300/3064\n",
      "1400/3064\n",
      "1500/3064\n",
      "1600/3064\n",
      "1700/3064\n",
      "1800/3064\n",
      "1900/3064\n",
      "2000/3064\n",
      "2100/3064\n",
      "2200/3064\n",
      "2300/3064\n",
      "2400/3064\n",
      "2500/3064\n",
      "2600/3064\n",
      "2700/3064\n",
      "2800/3064\n",
      "2900/3064\n",
      "3000/3064\n",
      "(3064, 8192)\n",
      "0.9003267973856209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vicuna_7b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1ab1a9ba094558b5535fae9b0c6f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3064\n",
      "100/3064\n",
      "200/3064\n",
      "300/3064\n",
      "400/3064\n",
      "500/3064\n",
      "600/3064\n",
      "700/3064\n",
      "800/3064\n",
      "900/3064\n",
      "1000/3064\n",
      "1100/3064\n",
      "1200/3064\n",
      "1300/3064\n",
      "1400/3064\n",
      "1500/3064\n",
      "1600/3064\n",
      "1700/3064\n",
      "1800/3064\n",
      "1900/3064\n",
      "2000/3064\n",
      "2100/3064\n",
      "2200/3064\n",
      "2300/3064\n",
      "2400/3064\n",
      "2500/3064\n",
      "2600/3064\n",
      "2700/3064\n",
      "2800/3064\n",
      "2900/3064\n",
      "3000/3064\n",
      "(3064, 4096)\n",
      "0.8937908496732027\n",
      "vicuna_13b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b9465495fd49e89ab7e45e459412df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3064\n",
      "100/3064\n",
      "200/3064\n",
      "300/3064\n",
      "400/3064\n",
      "500/3064\n",
      "600/3064\n",
      "700/3064\n",
      "800/3064\n",
      "900/3064\n",
      "1000/3064\n",
      "1100/3064\n",
      "1200/3064\n",
      "1300/3064\n",
      "1400/3064\n",
      "1500/3064\n",
      "1600/3064\n",
      "1700/3064\n",
      "1800/3064\n",
      "1900/3064\n",
      "2000/3064\n",
      "2100/3064\n",
      "2200/3064\n",
      "2300/3064\n",
      "2400/3064\n",
      "2500/3064\n",
      "2600/3064\n",
      "2700/3064\n",
      "2800/3064\n",
      "2900/3064\n",
      "3000/3064\n",
      "(3064, 5120)\n",
      "0.8872549019607843\n",
      "vicuna_33b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5472ab9416f34a5a8507642e515484e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3064\n",
      "100/3064\n",
      "200/3064\n",
      "300/3064\n",
      "400/3064\n",
      "500/3064\n",
      "600/3064\n",
      "700/3064\n",
      "800/3064\n",
      "900/3064\n",
      "1000/3064\n",
      "1100/3064\n",
      "1200/3064\n",
      "1300/3064\n",
      "1400/3064\n",
      "1500/3064\n",
      "1600/3064\n",
      "1700/3064\n",
      "1800/3064\n",
      "1900/3064\n",
      "2000/3064\n",
      "2100/3064\n",
      "2200/3064\n",
      "2300/3064\n",
      "2400/3064\n",
      "2500/3064\n",
      "2600/3064\n",
      "2700/3064\n",
      "2800/3064\n",
      "2900/3064\n",
      "3000/3064\n",
      "(3064, 6656)\n",
      "0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "for model_name in model_name_to_meta_data:\n",
    "    print(model_name)\n",
    "    meta_data = model_name_to_meta_data[model_name]\n",
    "    \n",
    "    if 'llama' in model_name or 'vicuna' in model_name:\n",
    "        tokenizer = meta_data['tokenizer_class'].from_pretrained(meta_data['tokenizer_path'], legacy=False)\n",
    "    else:\n",
    "        tokenizer = meta_data['tokenizer_class'].from_pretrained(meta_data['tokenizer_path'])\n",
    "    \n",
    "    if meta_data['load_in_4bit']:\n",
    "        model =  meta_data['model_class'].from_pretrained(meta_data['model_path'], device_map='auto', load_in_4bit=meta_data['load_in_4bit'])\n",
    "    else:\n",
    "        model =  meta_data['model_class'].from_pretrained(meta_data['model_path']).to(device)\n",
    "    \n",
    "    # actual generation\n",
    "    last_token_embeddings = []\n",
    "    mean_pooling_embeddings = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if index % 100 == 0:\n",
    "            print(f'{index}/{len(df)}')\n",
    "\n",
    "        encoded_input = tokenizer(row['response'], return_tensors='pt').to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "\n",
    "        last_token_embedding = last_token(model_output)\n",
    "        mean_pooling_embedding = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "        last_token_embeddings.append(last_token_embedding.cpu())\n",
    "        mean_pooling_embeddings.append(mean_pooling_embedding.cpu())\n",
    "\n",
    "    last_token_embeddings = np.concatenate(last_token_embeddings)\n",
    "    mean_pooling_embeddings = np.concatenate(mean_pooling_embeddings)\n",
    "    \n",
    "    print(np.shape(mean_pooling_embeddings))\n",
    "    \n",
    "    # sanity check\n",
    "    train_df = df[df.subset == 'train'].copy()\n",
    "    validate_df = df[df.subset == 'validate'].copy()\n",
    "\n",
    "    validate_df['predicted'] = 0\n",
    "    for qid, sub_train_df in train_df.groupby('qid'):\n",
    "        lr = LogisticRegression(max_iter=1000000)\n",
    "\n",
    "        train_X = np.zeros((len(sub_train_df), np.shape(mean_pooling_embeddings)[1]))\n",
    "        train_y = np.zeros((len(sub_train_df), ), dtype=int)\n",
    "        index = 0\n",
    "        for i, row in sub_train_df.iterrows():\n",
    "            train_X[index, :] = mean_pooling_embeddings[i]\n",
    "            train_y[index] = row.binary_ground_truth\n",
    "\n",
    "            index += 1\n",
    "\n",
    "        lr.fit(train_X, train_y)\n",
    "\n",
    "        targets = np.array(train_y)\n",
    "        predicted = np.array(lr.predict(train_X))\n",
    "\n",
    "        sub_validate_df = validate_df[validate_df.qid == qid]\n",
    "\n",
    "        validate_X = np.zeros((len(sub_validate_df), np.shape(mean_pooling_embeddings)[1]))\n",
    "        validate_y = np.zeros((len(sub_validate_df), ), dtype=int)\n",
    "        index = 0\n",
    "        for i, row in sub_validate_df.iterrows():\n",
    "            validate_X[index, :] = mean_pooling_embeddings[i]\n",
    "            validate_y[index] = row.binary_ground_truth\n",
    "\n",
    "            index += 1\n",
    "\n",
    "        targets = np.array(validate_y)\n",
    "        predicted = lr.predict(validate_X)\n",
    "\n",
    "        validate_df.loc[sub_validate_df.index, 'predicted'] = predicted\n",
    "        \n",
    "    print(len(validate_df[validate_df.predicted == validate_df.binary_ground_truth]) / len(validate_df))\n",
    "    \n",
    "    pickle.dump(last_token_embeddings.tolist(), open(f'embeddings/{model_name}_last_token.pkl', 'wb'))\n",
    "    pickle.dump(mean_pooling_embeddings.tolist(), open(f'embeddings/{model_name}_mean_pooling.pkl', 'wb'))\n",
    "    \n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca74fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
