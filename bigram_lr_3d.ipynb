{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b61e0e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import itertools\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from spellchecker import SpellChecker\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b13a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unescape_response(response):\n",
    "    response = unidecode(response)\n",
    "    prev_response = response\n",
    "    response = html.unescape(response)\n",
    "    while prev_response != response:\n",
    "        prev_response = response\n",
    "        response = html.unescape(response)\n",
    "\n",
    "    return response\n",
    "\n",
    "def tokenize(response):\n",
    "    response = str(response)\n",
    "    \n",
    "    spell_checker = SpellChecker()\n",
    "    spell_checker.distance = 1\n",
    "    \n",
    "    punctuations = ' ,.:;?'\n",
    "    preprocessed_response = []\n",
    "    for i in range(len(response)):\n",
    "        if response[i] in [\"'\", '\"']:\n",
    "            if i != 0 and i != len(response) - 1:\n",
    "                if response[i - 1] not in punctuations and response[i + 1] not in punctuations:\n",
    "                    preprocessed_response.append(response[i])\n",
    "        else:\n",
    "            preprocessed_response.append(response[i])\n",
    "    response = ''.join(preprocessed_response)\n",
    "\n",
    "    operators = '+-*/%=<>()[]{}#'\n",
    "    preprocessed_response = []\n",
    "    for i in range(len(response)):\n",
    "        if response[i] in operators:\n",
    "            if i != 0 and response[i - 1] != ' ':\n",
    "                preprocessed_response.append(' ')\n",
    "            preprocessed_response.append(response[i])\n",
    "            if i != len(response) - 1 and response[i + 1] != ' ':\n",
    "                preprocessed_response.append(' ')\n",
    "        else:\n",
    "            preprocessed_response.append(response[i])\n",
    "\n",
    "    response = ''.join(preprocessed_response)\n",
    "\n",
    "    spell_checker_skip = [\"it's\"]\n",
    "    tokens_list = []\n",
    "    for sentence in nltk.sent_tokenize(response):\n",
    "        nltk_tokens = nltk.word_tokenize(sentence)\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(nltk_tokens):\n",
    "            if i < len(nltk_tokens) - 1 and nltk_tokens[i + 1][0] == \"'\":\n",
    "                tokens.append(nltk_tokens[i].lower() + nltk_tokens[i + 1].lower())\n",
    "                i += 1\n",
    "            elif nltk_tokens[i] not in punctuations:\n",
    "                tokens.append(nltk_tokens[i].lower())\n",
    "            i += 1\n",
    "        \n",
    "        tokens_list.append([(spell_checker.correction(token) if spell_checker.correction(token) is not None else token) \n",
    "                            if token not in spell_checker_skip else token \n",
    "                            for token in tokens])\n",
    "\n",
    "    return tokens_list\n",
    "\n",
    "def initialize_bow(tokens_lists, percentage_threshold=0.6):\n",
    "    token_to_count = {}\n",
    "    for token_list in tokens_lists:\n",
    "        tokens = set(itertools.chain.from_iterable(token_list))\n",
    "        for token in tokens:\n",
    "            if token not in token_to_count:\n",
    "                token_to_count[token] = 0\n",
    "            token_to_count[token] += 1\n",
    "            \n",
    "    sorted_counts = sorted(token_to_count.values())\n",
    "    threshold = sorted_counts[int(len(sorted_counts) * percentage_threshold)]\n",
    "    \n",
    "    token_to_index = {}\n",
    "    index = 0\n",
    "    for token in token_to_count:\n",
    "        if token_to_count[token] >= threshold:\n",
    "            token_to_index[token] = index\n",
    "            index += 1\n",
    "    \n",
    "    return token_to_index, len(token_to_index)\n",
    "\n",
    "def bowize(token_to_index, tokens_list):\n",
    "    tokens = set(itertools.chain.from_iterable(tokens_list))\n",
    "    \n",
    "    X = np.zeros((1, len(token_to_index)))\n",
    "    for token in tokens:\n",
    "        if token in token_to_index:\n",
    "            X[0, token_to_index[token]] = 1\n",
    "    \n",
    "    return X\n",
    "\n",
    "def initialize_bigram(tokens_lists, percentage_threshold=0.8):\n",
    "    bigram_to_count = {}\n",
    "    for tokens_list in tokens_lists:\n",
    "        bigrams = []\n",
    "        for tokens in tokens_list:\n",
    "            bigrams.extend(map(lambda x : x[0] + ' ' + x[1], zip(tokens[0:-1], tokens[1:])))\n",
    "        \n",
    "        bigrams = set(bigrams)\n",
    "        for bigram in bigrams:\n",
    "            if bigram not in bigram_to_count:\n",
    "                bigram_to_count[bigram] = 0\n",
    "            bigram_to_count[bigram] += 1\n",
    "    \n",
    "    sorted_counts = sorted(bigram_to_count.values())\n",
    "    threshold = sorted_counts[int(len(sorted_counts) * percentage_threshold)]\n",
    "    \n",
    "    bigram_to_index = {}\n",
    "    index = 0\n",
    "    for bigram in bigram_to_count:\n",
    "        if bigram_to_count[bigram] >= threshold:\n",
    "            bigram_to_index[bigram] = index\n",
    "            index += 1\n",
    "    \n",
    "    token_to_index, _ = initialize_bow(tokens_lists)\n",
    "    \n",
    "    return (token_to_index, bigram_to_index), len(token_to_index) + len(bigram_to_index)\n",
    "    \n",
    "def bigramize(to_index_tuple, tokens_list):\n",
    "    token_to_index, bigram_to_index = to_index_tuple\n",
    "    \n",
    "    X = np.zeros((1, len(token_to_index) + len(bigram_to_index)))\n",
    "    \n",
    "    tokens = set(itertools.chain.from_iterable(tokens_list))\n",
    "    for token in tokens:\n",
    "        if token in token_to_index:\n",
    "            X[0, token_to_index[token]] = 1\n",
    "    \n",
    "    for tokens in tokens_list:\n",
    "        for bigram in map(lambda x : x[0] + ' ' + x[1], zip(tokens[0:-1], tokens[1:])):\n",
    "            if bigram in bigram_to_index:\n",
    "                X[0, len(token_to_index) + bigram_to_index[bigram]] = 1\n",
    "    \n",
    "    return X\n",
    "\n",
    "def initialize_trigram(tokens_lists, percentage_threshold=0.9):\n",
    "    trigram_to_count = {}\n",
    "    for tokens_list in tokens_lists:\n",
    "        trigrams = []\n",
    "        for tokens in tokens_list:\n",
    "            trigrams.extend(map(lambda x : x[0] + ' ' + x[1] + ' ' + x[2], zip(tokens[0:-2], tokens[1:-1], tokens[2:])))\n",
    "        \n",
    "        trigrams = set(trigrams)\n",
    "        for trigram in trigrams:\n",
    "            if trigram not in trigram_to_count:\n",
    "                trigram_to_count[trigram] = 0\n",
    "            trigram_to_count[trigram] += 1\n",
    "    \n",
    "    sorted_counts = sorted(trigram_to_count.values())\n",
    "    threshold = sorted_counts[int(len(sorted_counts) * percentage_threshold)]\n",
    "    \n",
    "    trigram_to_index = {}\n",
    "    index = 0\n",
    "    for trigram in trigram_to_count:\n",
    "        if trigram_to_count[trigram] >= threshold:\n",
    "            trigram_to_index[trigram] = index\n",
    "            index += 1\n",
    "    \n",
    "    (token_to_index, bigram_to_index), _ = initialize_bigram(tokens_lists)\n",
    "    \n",
    "    return (token_to_index, bigram_to_index, trigram_to_index), len(token_to_index) + len(bigram_to_index) + len(trigram_to_index)\n",
    "\n",
    "def trigramize(to_index_tuple, tokens_list):\n",
    "    token_to_index, bigram_to_index, trigram_to_index = to_index_tuple\n",
    "    \n",
    "    X = np.zeros((1, len(token_to_index) + len(bigram_to_index) + len(trigram_to_index)))\n",
    "    \n",
    "    tokens = set(itertools.chain.from_iterable(tokens_list))\n",
    "    for token in tokens:\n",
    "        if token in token_to_index:\n",
    "            X[0, token_to_index[token]] = 1\n",
    "    \n",
    "    for tokens in tokens_list:\n",
    "        for bigram in map(lambda x : x[0] + ' ' + x[1], zip(tokens[0:-1], tokens[1:])):\n",
    "            if bigram in bigram_to_index:\n",
    "                X[0, len(token_to_index) + bigram_to_index[bigram]] = 1\n",
    "                \n",
    "    for tokens in tokens_list:\n",
    "        for trigram in map(lambda x : x[0] + ' ' + x[1] + ' ' + x[2], zip(tokens[0:-2], tokens[1:-1], tokens[2:])):\n",
    "            if trigram in trigram_to_index:\n",
    "                X[0, len(token_to_index) + len(bigram_to_index) + trigram_to_index[trigram]] = 1\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6d66e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sigcse_2024.csv')\n",
    "\n",
    "train_df = df[df.subset == 'train'].copy()\n",
    "validate_df = df[df.subset == 'validate'].copy()\n",
    "test_df = df[df.subset == 'test'].copy()\n",
    "\n",
    "train_df['tokens_list'] = train_df.response.apply(tokenize)\n",
    "validate_df['tokens_list'] = validate_df.response.apply(tokenize)\n",
    "test_df['tokens_list'] = test_df.response.apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cd24731",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for suffix in ['una', 'c', 'hl']:\n",
    "    test_df[f'bigram_{suffix}'] = 0\n",
    "    for qid, sub_train_df in train_df.groupby('qid'):\n",
    "        if len(sub_train_df[suffix].unique()) == 1:\n",
    "            sub_validate_df = validate_df[validate_df.qid == qid]\n",
    "            sub_test_df = test_df[test_df.qid == qid]\n",
    "            \n",
    "            validate_df.loc[sub_validate_df.index, f'bigram_{suffix}'] = sub_train_df[suffix].unique()[0]\n",
    "            test_df.loc[sub_test_df.index, f'bigram_{suffix}'] = sub_train_df[suffix].unique()[0]\n",
    "        else:\n",
    "            lr = LogisticRegression()\n",
    "\n",
    "            feature_map, feature_size = initialize_bigram(sub_train_df.tokens_list)\n",
    "\n",
    "            train_X = np.zeros((len(sub_train_df), feature_size))\n",
    "            train_y = np.zeros((len(sub_train_df), ), dtype=int)\n",
    "            index = 0\n",
    "            for _, row in sub_train_df.iterrows():\n",
    "                train_X[index, :] = bigramize(feature_map, row.tokens_list)\n",
    "                train_y[index] = row[suffix]\n",
    "\n",
    "                index += 1\n",
    "\n",
    "            lr.fit(train_X, train_y)\n",
    "            \n",
    "            sub_validate_df = validate_df[validate_df.qid == qid]\n",
    "            validate_X = np.zeros((len(sub_validate_df), feature_size))\n",
    "            validate_y = np.zeros((len(sub_validate_df), ), dtype=int)\n",
    "            index = 0\n",
    "            for _, row in sub_validate_df.iterrows():\n",
    "                validate_X[index, :] = bigramize(feature_map, row.tokens_list)\n",
    "                validate_y[index] = row[suffix]\n",
    "\n",
    "                index += 1\n",
    "            predicted = lr.predict(validate_X)\n",
    "            validate_df.loc[sub_validate_df.index, f'bigram_{suffix}'] = predicted\n",
    "\n",
    "            sub_test_df = test_df[test_df.qid == qid]\n",
    "\n",
    "            test_X = np.zeros((len(sub_test_df), feature_size))\n",
    "            test_y = np.zeros((len(sub_test_df), ), dtype=int)\n",
    "            index = 0\n",
    "            for _, row in sub_test_df.iterrows():\n",
    "                test_X[index, :] = bigramize(feature_map, row.tokens_list)\n",
    "                test_y[index] = row[suffix]\n",
    "\n",
    "                index += 1\n",
    "\n",
    "            predicted = lr.predict(test_X)\n",
    "            test_df.loc[sub_test_df.index, f'bigram_{suffix}'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4984ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8d69b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n",
      "0.8612903225806452\n",
      "0.7186190591786548\n",
      "0.8760806916426513\n",
      "una\n",
      "0.7306451612903225\n",
      "0.33423353909465014\n",
      "0.8125701459034792\n",
      "hl\n",
      "0.8951612903225806\n",
      "0.5917747163695299\n",
      "0.9382716049382717\n"
     ]
    }
   ],
   "source": [
    "for suffix in ['c', 'una', 'hl']:\n",
    "    print(suffix)\n",
    "    print(accuracy_score(test_df[suffix], test_df[f'bigram_{suffix}']))\n",
    "    print(cohen_kappa_score(test_df[suffix], test_df[f'bigram_{suffix}']))\n",
    "    print(f1_score(test_df[suffix], test_df[f'bigram_{suffix}']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23fdc680",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_df['c_una_hl'] = validate_df.apply(lambda row : f'{row.c}_{row.una}_{row.hl}', axis=1)\n",
    "validate_df['c_una_hl_predicted'] = validate_df.apply(lambda row : f'{int(row.bigram_c)}_{int(row.bigram_una)}_{int(row.bigram_hl)}', axis=1)\n",
    "validate_df['converted_binary_ground_truth'] = validate_df.c_una_hl.apply(lambda x : 1 if x == '1_1_1' else 0)\n",
    "validate_df['converted_binary_predicted'] = validate_df.c_una_hl_predicted.apply(lambda x : 1 if x == '1_1_1' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54588131",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['c_una_hl'] = test_df.apply(lambda row : f'{row.c}_{row.una}_{row.hl}', axis=1)\n",
    "test_df['c_una_hl_predicted'] = test_df.apply(lambda row : f'{int(row.bigram_c)}_{int(row.bigram_una)}_{int(row.bigram_hl)}', axis=1)\n",
    "test_df['converted_binary_ground_truth'] = test_df.c_una_hl.apply(lambda x : 1 if x == '1_1_1' else 0)\n",
    "test_df['converted_binary_predicted'] = test_df.c_una_hl_predicted.apply(lambda x : 1 if x == '1_1_1' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85965427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8354838709677419\n",
      "0.6595423956931359\n",
      "0.7968127490039841\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test_df['converted_binary_ground_truth'], test_df['converted_binary_predicted']))\n",
    "print(cohen_kappa_score(test_df['converted_binary_ground_truth'], test_df['converted_binary_predicted']))\n",
    "print(f1_score(test_df['converted_binary_ground_truth'], test_df['converted_binary_predicted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68a79564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_predict(row, order):\n",
    "    if row.converted_binary_ground_truth == 1:\n",
    "        return row.converted_binary_predicted\n",
    "    else:\n",
    "        for suffix in order:\n",
    "            if row[suffix] == 1:\n",
    "                if row[f'bigram_{suffix}'] == 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    return -1\n",
    "            else:\n",
    "                if row[f'bigram_{suffix}'] == 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    return 0\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce705115",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = [\n",
    "    ['c', 'hl', 'una'], \n",
    "    ['c', 'una', 'hl'],\n",
    "    ['hl', 'c', 'una'],\n",
    "    ['hl', 'una', 'c'],\n",
    "    ['una', 'c', 'hl'],\n",
    "    ['una', 'hl', 'c']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d4a7602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c', 'hl', 'una']\n",
      "0.7973856209150327\n",
      "0.5878384983869391\n",
      "0.7669172932330828\n",
      "['c', 'una', 'hl']\n",
      "0.7973856209150327\n",
      "0.5878384983869391\n",
      "0.7669172932330828\n",
      "['hl', 'c', 'una']\n",
      "0.7892156862745098\n",
      "0.5720326119953163\n",
      "0.7597765363128491\n",
      "['hl', 'una', 'c']\n",
      "0.7549019607843137\n",
      "0.5062976627120285\n",
      "0.7311827956989249\n",
      "['una', 'c', 'hl']\n",
      "0.7467320261437909\n",
      "0.49079939020462504\n",
      "0.7246891651865008\n",
      "['una', 'hl', 'c']\n",
      "0.75\n",
      "0.49699170570286644\n",
      "0.7272727272727272\n"
     ]
    }
   ],
   "source": [
    "for order in orders:\n",
    "    order_string = '_'.join(order)\n",
    "    validate_df[order_string] = validate_df.apply(lambda row :  order_predict(row, order), axis=1)\n",
    "    validate_df[f'{order_string}_ground_truth'] = validate_df['converted_binary_ground_truth']\n",
    "    validate_df.loc[validate_df[order_string] == -1, f'{order_string}_ground_truth'] = 1\n",
    "    validate_df.loc[validate_df[order_string] == -1, order_string] = 0\n",
    "    \n",
    "    print(order)\n",
    "    print(accuracy_score(validate_df[f'{order_string}_ground_truth'], validate_df[order_string]))\n",
    "    print(cohen_kappa_score(validate_df[f'{order_string}_ground_truth'], validate_df[order_string]))\n",
    "    print(f1_score(validate_df[f'{order_string}_ground_truth'], validate_df[order_string]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3c83c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5854838709677419\n",
      "0.4500491828740055\n",
      "0.5733543887217953\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test_df['c_una_hl'], test_df['c_una_hl_predicted']))\n",
    "print(cohen_kappa_score(test_df['c_una_hl'], test_df['c_una_hl_predicted']))\n",
    "print(f1_score(test_df['c_una_hl'], test_df['c_una_hl_predicted'], average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d725bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c', 'una', 'hl']\n",
      "0.7677419354838709\n",
      "0.5285220899652517\n",
      "0.7352941176470588\n"
     ]
    }
   ],
   "source": [
    "order = ['c', 'una', 'hl']\n",
    "order_string = '_'.join(order)\n",
    "test_df[order_string] = test_df.apply(lambda row :  order_predict(row, order), axis=1)\n",
    "test_df[f'{order_string}_ground_truth'] = test_df['converted_binary_ground_truth']\n",
    "test_df.loc[test_df[order_string] == -1, f'{order_string}_ground_truth'] = 1\n",
    "test_df.loc[test_df[order_string] == -1, order_string] = 0\n",
    "\n",
    "print(order)\n",
    "print(accuracy_score(test_df[f'{order_string}_ground_truth'], test_df[order_string]))\n",
    "print(cohen_kappa_score(test_df[f'{order_string}_ground_truth'], test_df[order_string]))\n",
    "print(f1_score(test_df[f'{order_string}_ground_truth'], test_df[order_string]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5cb29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
